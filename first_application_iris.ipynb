{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First Machine Learning Application\n",
    "In this application, we will be classifying the iris dataset. This dataset is about classifying the species of iris flowers whose measurement have been collected and labelled in the dataset. The measurements collected are the length and width of the sepals and length and width of the petals. The measurements are in centimeters. \n",
    "The species of the flowers whose data has been recorded are **setosa, versicolor, and virginica**. \n",
    "\n",
    "<img src=\"./iris_image_2.png\">\n",
    "\n",
    "Because we have measurements for which we know the correct species of iris, this is a **supervised learning problem**. In this problem we want to predict one of the several options (the species of iris). This is an example of a **classification problem**. The possible outputs are called classes. So this is a three class problem as every iris in the dataset belongs to these three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and PreProcessing\n",
    "We are not going to focus on data collection on our first example, so we will be using the dataset provided by the scikit-learn library which is already collected, cleaned and structured for us so that we can focus on other stuff like shich algorithm to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset returned by the load_iris function is a *Bunch* object which is very similar to a dictionary. It contains keys and volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the DataSet\n",
    "Here we will explore the dataset to see what it contains and how we can use and process it to our advantage. This step is very important as it gives us crucial insights on how to classify this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of iris_dataset:\n",
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys of iris_dataset:\\n{}\".format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'DESCR' key contains an short description of the dataset of which the first 200 lines are displayed below. Feel free to check it out more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['DESCR'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the key 'target_names' is an array of strings, containing the species of the flowers that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of 'feature_names' is a list of strings, giving the description of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: \n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Names: \\n{}\".format(iris_dataset['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data itself is contained is contained in the 'data' and 'target' fields, data contains the numeric measurements of the sepal length, sepal width, petal length, and petal width in a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of data: {}\".format(type(iris_dataset['data'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows in the data array corresponds to the number of flowers whose measurement has been taken, while the columns represent the four measurements that were taken for each flower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: {}\".format(iris_dataset['data'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the dataset contains 150 flower examples (which is called *samples* in machine learning), each example containing 4 measurements (called *features*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the feature value for the first 5 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Five Samples: \n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First Five Samples: \\n{}\".format(iris_dataset['data'][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target array contains the species of each of the flowers that were measured, also in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Target: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of Target: {}\".format(type(iris_dataset['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Target: (150,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Target: {}\".format(iris_dataset['target'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The species are encoded as integers from 0 to 2\n",
    "0. setosa\n",
    "1. versicolor\n",
    "2. virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target: \\n{}\".format(iris_dataset['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing\n",
    "We also want to set some kind of metric which tests how much our machine learning model has actually learned. Its just like giving exams, and as in the case of exams, we cannot use the examples in which we have taught the system. Therefore we split the dataset into two parts called **training dataset and testing dataset**. We train our model on the training dataset and then test its performance on the testing dataset.\n",
    "scikit-learn comes with a function that shuffles the dataset and splits it into the training and testing datasets. This function is called the **train_test_split** and by default it splits in the ratio of **0.75:0.25**. That is 75% of the data is given to training set and 25% of the data is given to testing set. This ratio can be changed to suit your needs too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(iris_dataset['data'],iris_dataset['target'],random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the X_train and y_train are the data and target features of the training set, similarly the X_test and y_test are the data and target features of the testing set. The random_state parameter sets the random seed so that every time you use a particular random_state, you get the same distribution of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Having a look at the data\n",
    "It is a good idea to have a look at the data statistically as this gives important insights. And the best way to do it is by visualizing it. One way to visualize it is by using a scatter plot. A scatter plot maps every feature with every other feature taking two at a time. Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mglearn\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15,15),marker='o',hist_kwds={'bins':20},\n",
    "                       s=60,alpha=0.8,cmap = mglearn.cm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if the parameters scare you out a bit, for now just think of this as a piece of magic code which will let you create this plot. You will understand everything in no time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your first model: k-Nearest Neighbors\n",
    "Now we are going to use k-nearest neighbors classifier, also called knn classifier to calssify our dataset, but before we have to train it. Now training the knn algorithm is really easy to understand, as this algorithm only stores the training set. To make a prediction for a new data point, the algorithm finds the point in the training set that is closest to the new point. Then it assigns the label of this training point to the new data point\n",
    "The **k** in **k-nearest neighbors** signifies that instead of using only one neighbor, we will be using k nearest neighbors and then make the prediction based on the majority of labels of the k nearest neighbors. This is the only parameter we need to set while training this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3) #Setting the no of neighbors to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "knn.fit(X_train,y_train) #This fit method trains our model on the given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model on the test dataset:\n",
    "Now we will evaluate the performance of the model on the test dataset. We will measure the **accuracy** of the model, which is the fraction of flowers for which the right species was predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 1 0 2 0 0 1 1 2 2 0 1 2 1 0 2 2 1 2 1 0 1 0 0 1 2 0 2 2 0 2 1 1\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(y_pred) # prints the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test,y_test) #This prints the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we can observe that our testing accuracy is approximately 95%, 94.736% to be precise upto 3 digits after decimal. Congratulations on building your first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
